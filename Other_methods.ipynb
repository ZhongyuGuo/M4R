{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Other_methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhongyuGuo/M4R/blob/main/Other_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqcGnZQppIPs"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import sklearn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcY44nw_pN3F",
        "outputId": "a37ee31f-428c-4abb-ad5c-4e6b46d9bf2f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "df=pd.read_csv(\"/content/drive/My Drive/M4R/papers2019_200000.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mSyUsEX5TNW"
      },
      "source": [
        "TRAIN_FRACTION=0.7"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCqdZoav5IW5"
      },
      "source": [
        "# List of categories\n",
        "catlist=['math','cs','quant-ph','hep-ex','hep-th','astro-ph','nucl-ex','nucl-th']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so2HLean4pb_"
      },
      "source": [
        "def format_text(text):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "\n",
        "    # Add spaces around punctuation\n",
        "    text = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', text)\n",
        "\n",
        "    # Remove references to figures\n",
        "    text = re.sub(r'\\((\\d+)\\)', r'', text)\n",
        "\n",
        "    text=re.sub(' the ',' ',text)\n",
        "    text=re.sub(' of ',' ',text)\n",
        "    text=re.sub(' and ',' ',text)\n",
        "    text=re.sub(' a ',' ',text)\n",
        "    text=re.sub(' in ',' ',text)\n",
        "    text=re.sub(' to ',' ',text)\n",
        "    text=re.sub(' with ',' ',text)\n",
        "    text=re.sub(' for ',' ',text)\n",
        "    text=re.sub(' by ',' ',text)\n",
        "    text=re.sub(' on ',' ',text)\n",
        "    text=re.sub(' as ',' ',text)\n",
        "    text=re.sub(' an ',' ',text)\n",
        "    text=re.sub(' at ',' ',text)\n",
        "    text=re.sub(' we ',' ',text)\n",
        "    text=re.sub(' is ',' ',text)\n",
        "    text=re.sub(' this ',' ',text)\n",
        "    text=re.sub(' are ',' ',text)\n",
        "    text=re.sub(' which ',' ',text)\n",
        "    text=re.sub(' be ',' ',text)\n",
        "    text=re.sub(' it ',' ',text)\n",
        "    text=re.sub(' that ',' ',text)\n",
        "    text=re.sub(' from ',' ',text)\n",
        "    text=re.sub(' can ',' ',text)\n",
        "    text=re.sub(' these ',' ',text)\n",
        "    text=re.sub(' our ',' ',text)\n",
        "    text=re.sub(' has ',' ',text)\n",
        "    text=re.sub(' have ',' ',text)\n",
        "    text=re.sub('.We ','.',text)\n",
        "    text=re.sub('.That ','.',text)\n",
        "    text=re.sub('.The ','.',text)\n",
        "    text=re.sub('.From ','.',text)\n",
        "    text=re.sub('.Our ','.',text)\n",
        "    text=re.sub('.In ','.',text)\n",
        "    text=re.sub('.These ','.',text)\n",
        "    text=re.sub('.This ','.',text)\n",
        "    text=re.sub(',that ',',',text)\n",
        "\n",
        "    # Remove double spaces\n",
        "    text = re.sub(r'\\s\\s', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLZcRzET4sUE"
      },
      "source": [
        "def make_sequences(texts,\n",
        "                   training_length=50,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    features = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    categories=list(set(text['categories'].values))\n",
        "    labels=np.zeros(len(text))\n",
        "    for i in range(len(text)):\n",
        "      ind=categories.index(text.iloc[i,1])\n",
        "      labels[i]=ind\n",
        "    return word_idx, idx_word, num_words, word_counts, features,labels\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vQ6gZjWttg_"
      },
      "source": [
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       train_fraction=TRAIN_FRACTION):\n",
        "    \"\"\"Create training and validation features and labels.\"\"\"\n",
        "\n",
        "    # Randomly shuffle features and labels\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Decide on number of samples for training\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "    y_train, y_valid = np.array(train_labels), np.array(valid_labels)\n",
        "\n",
        "    # Memory management\n",
        "    import gc\n",
        "    gc.enable()\n",
        "    del features, train_features, labels, valid_features, train_labels, valid_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e8OKjh1tyuJ"
      },
      "source": [
        "RANDOM_STATE = 50"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqc837yi5Jbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e170552-03cc-41b5-bec1-465f24d7b1d0"
      },
      "source": [
        "# Categories to classify\n",
        "train_acc_list_lr=[]\n",
        "test_acc_list_lr=[]\n",
        "train_acc_list_rf=[]\n",
        "test_acc_list_rf=[]\n",
        "train_acc_list_svm=[]\n",
        "test_acc_list_svm=[]\n",
        "f1_lr=[]\n",
        "f1_rf=[]\n",
        "f1_svm=[]\n",
        "cat1=catlist[1]\n",
        "df1=df[df['categories']==cat1][0:1500].reset_index(drop=True)\n",
        "for j in range(2,8):\n",
        "  print(j)\n",
        "  cat2=catlist[j]\n",
        "  # Obtain corresponding abstarcts\n",
        "  df2=df[df['categories']==cat2][0:1500].reset_index(drop=True)\n",
        "  text=df1.append(df2).reset_index(drop=True)\n",
        "  formatted = []\n",
        "\n",
        "  # Iterate through all the original abstracts\n",
        "  for a in text['abstracts']:\n",
        "      formatted.append(format_text(a))\n",
        "  TRAINING_LENGTH = 50\n",
        "  filters = '.,!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "  word_idx, idx_word, num_words, word_counts,features,labels = make_sequences(\n",
        "      formatted, TRAINING_LENGTH, lower=True, filters=filters)\n",
        "  #Make all sequences to same length\n",
        "  pad = len(max(features, key=len))\n",
        "  features=[i + [0]*(pad-len(i)) for i in features]\n",
        "  features1=np.array(features)\n",
        "  # Define Training and Test sets\n",
        "  X_input=np.zeros([2000,len(features[0])])\n",
        "  X_input[0:1000,:]=features1[0:1000,:]\n",
        "  X_input[1000:2000]=features1[1500:2500,:]\n",
        "  X_test=np.zeros([1000,len(features[0])])\n",
        "  X_test[0:500,:]=features1[1000:1500,:]\n",
        "  X_test[500:1000]=features1[2500:3000,:]\n",
        "  y_input=np.zeros(2000)\n",
        "  y_input[0:1000]=labels[0:1000]\n",
        "  y_input[1000:2000]=labels[1500:2500]\n",
        "  y_test=np.zeros([1000])\n",
        "  y_test[0:500]=labels[1000:1500]\n",
        "  y_test[500:1000]=labels[2500:3000]\n",
        "  from sklearn.utils import shuffle\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = create_train_valid(X_input, y_input)\n",
        "  #Preprecoess the training data and test features\n",
        "  from sklearn import preprocessing\n",
        "  scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "  X_train_scaled=scaler.transform(X_train)\n",
        "  scaler1 = preprocessing.StandardScaler().fit(X_test)\n",
        "  X_test_scaled=scaler1.transform(X_test)\n",
        "\n",
        "  #LR\n",
        "  # Parameters to search for the optimal ones\n",
        "  parameters_LR = {'C':np.linspace(0.001,0.01,19),'solver':['lbfgs','sag','liblinear','newton-cg']}\n",
        "  #GridSeach for Logistic Regression\n",
        "  lr=LogisticRegression(max_iter=200000)\n",
        "  lr_optimal=GridSearchCV(lr,parameters_LR,cv=5,n_jobs=4)\n",
        "  lr_optimal.fit(X_train_scaled,y_train)\n",
        "  print('Best params:',lr_optimal.best_params_)\n",
        "  #Save scores on train and test set\n",
        "  training_predict =lr_optimal.predict(X_train_scaled)\n",
        "  test_predict=lr_optimal.predict(X_test_scaled)\n",
        "  from sklearn.metrics import accuracy_score,f1_score\n",
        "  train_acc=accuracy_score(y_train,training_predict)\n",
        "  test_acc=accuracy_score(y_test,test_predict)\n",
        "  train_acc_list_lr.append(train_acc)\n",
        "  test_acc_list_lr.append(test_acc)\n",
        "  train_f1=f1_score(y_train,training_predict)\n",
        "  test_f1=f1_score(y_test,test_predict)\n",
        "  f1_lr.append(test_f1)\n",
        "\n",
        "  #RF\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  # Parameter to search for thr optimal ones\n",
        "  parameters_RF = {'n_estimators':np.linspace(10,120,12,dtype='int'),'max_depth':np.linspace(10,50,5,dtype='int'),'max_features':np.linspace(5,10,5,dtype='int')}\n",
        "  rfc=RandomForestClassifier()\n",
        "  rfc_optimal=GridSearchCV(rfc,parameters_RF,cv=5,n_jobs=8)\n",
        "  rfc_optimal.fit(X_train_scaled,y_train)\n",
        "  print('Hyperparameter with biggest impact:',rfc_optimal.best_params_)\n",
        "  #Show train and test accuracy\n",
        "  training_predict =rfc_optimal.predict(X_train_scaled)\n",
        "  test_predict=rfc_optimal.predict(X_test_scaled)\n",
        "  train_acc=accuracy_score(y_train,training_predict)\n",
        "  test_acc=accuracy_score(y_test,test_predict)\n",
        "  train_acc_list_rf.append(train_acc)\n",
        "  test_acc_list_rf.append(test_acc)\n",
        "  train_f1=f1_score(y_train,training_predict)\n",
        "  test_f1=f1_score(y_test,test_predict)\n",
        "  f1_rf.append(test_f1)\n",
        "\n",
        "  #SVM\n",
        "  from sklearn.svm import SVC\n",
        "  parameters_SVM = {'kernel':('linear','poly','rbf'),'degree':np.linspace(2,8,7),'gamma':('scale','auto'),'C':np.linspace(0.001,0.01,10)}\n",
        "  svm=SVC()\n",
        "  svm_optimal=GridSearchCV(svm,parameters_SVM,cv=5,n_jobs=8)\n",
        "  svm_optimal.fit(X_train_scaled,y_train)\n",
        "\n",
        "  print('Hyperparameter with biggest impact:',svm_optimal.best_params_)\n",
        "\n",
        "  training_predict =svm_optimal.predict(X_train_scaled)\n",
        "  test_predict=svm_optimal.predict(X_test_scaled)\n",
        "  train_acc=accuracy_score(y_train,training_predict)\n",
        "  test_acc=accuracy_score(y_test,test_predict)\n",
        "  train_acc_list_svm.append(train_acc)\n",
        "  test_acc_list_svm.append(test_acc)\n",
        "  train_f1=f1_score(y_train,training_predict)\n",
        "  test_f1=f1_score(y_test,test_predict)\n",
        "  f1_svm.append(test_f1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "There are 22693 unique words.\n",
            "Best params: {'C': 0.0015, 'solver': 'sag'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 40, 'max_features': 10, 'n_estimators': 110}\n",
            "Hyperparameter with biggest impact: {'C': 0.002, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "3\n",
            "There are 20146 unique words.\n",
            "Best params: {'C': 0.004, 'solver': 'lbfgs'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 30, 'max_features': 10, 'n_estimators': 50}\n",
            "Hyperparameter with biggest impact: {'C': 0.001, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "4\n",
            "There are 21583 unique words.\n",
            "Best params: {'C': 0.003, 'solver': 'lbfgs'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 20, 'max_features': 10, 'n_estimators': 120}\n",
            "Hyperparameter with biggest impact: {'C': 0.002, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "5\n",
            "There are 24426 unique words.\n",
            "Best params: {'C': 0.0025, 'solver': 'lbfgs'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 30, 'max_features': 8, 'n_estimators': 120}\n",
            "Hyperparameter with biggest impact: {'C': 0.002, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "6\n",
            "There are 20769 unique words.\n",
            "Best params: {'C': 0.0045000000000000005, 'solver': 'sag'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 20, 'max_features': 8, 'n_estimators': 110}\n",
            "Hyperparameter with biggest impact: {'C': 0.002, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n",
            "7\n",
            "There are 23096 unique words.\n",
            "Best params: {'C': 0.0025, 'solver': 'lbfgs'}\n",
            "Hyperparameter with biggest impact: {'max_depth': 50, 'max_features': 10, 'n_estimators': 100}\n",
            "Hyperparameter with biggest impact: {'C': 0.001, 'degree': 2.0, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_wxrl6e4KhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14bd7f09-3efb-4182-c794-bb5df5c62536"
      },
      "source": [
        "train_acc_list_lr"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6642857142857143,\n",
              " 0.875,\n",
              " 0.7435714285714285,\n",
              " 0.7042857142857143,\n",
              " 0.795,\n",
              " 0.7035714285714286]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZyB2D5n4YRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18d349f7-3376-443c-df2d-9d285e927db2"
      },
      "source": [
        "test_acc_list_lr"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.644, 0.837, 0.747, 0.589, 0.792, 0.639]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMvTyCYx4dnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2d2095-ff42-40ef-d4bd-22b3762bbea6"
      },
      "source": [
        "f1_lr"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6008968609865472,\n",
              " 0.8409756097560975,\n",
              " 0.761994355597366,\n",
              " 0.5641569459172853,\n",
              " 0.7999999999999999,\n",
              " 0.6130760986066452]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV5YGBCE4u0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d705873b-395e-48d0-978c-b1c6362af3ca"
      },
      "source": [
        "train_acc_list_rf"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Ibcp9r40n0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947e18ce-ac9f-45c0-d734-82185e7c5b6f"
      },
      "source": [
        "test_acc_list_rf"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.692, 0.878, 0.763, 0.65, 0.836, 0.724]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M07LJoNO45mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d32fa47-4f4e-46b1-8d17-7b201c7d5d1f"
      },
      "source": [
        "f1_rf"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7274336283185842,\n",
              " 0.8723849372384939,\n",
              " 0.7398463227222833,\n",
              " 0.6641074856046065,\n",
              " 0.8197802197802199,\n",
              " 0.7299412915851272]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hUltBk85AGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55ed245-0349-4324-825e-bee9c73d1e2f"
      },
      "source": [
        "train_acc_list_svm"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6728571428571428,\n",
              " 0.8721428571428571,\n",
              " 0.755,\n",
              " 0.7128571428571429,\n",
              " 0.8071428571428572,\n",
              " 0.6842857142857143]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcaYHuL45GxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefd6c44-f66e-45a9-c650-9f8ffd2d8a82"
      },
      "source": [
        "test_acc_list_svm"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.636, 0.843, 0.746, 0.577, 0.811, 0.648]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVkt1CIT5Lhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a04af83-6664-4191-aa56-cff0dc6210e0"
      },
      "source": [
        "f1_svm"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5687203791469195,\n",
              " 0.8480154888673765,\n",
              " 0.7665441176470587,\n",
              " 0.5387131952017448,\n",
              " 0.8123138033763654,\n",
              " 0.5981735159817352]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}