{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_by_author.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPns+how0SRZq6xoVxoymRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhongyuGuo/M4R/blob/main/Classification_Edery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXNbDHdsc2f3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras\n",
        "import random\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm4yNcvfc-f4",
        "outputId": "a6260ef0-3720-4d3d-f832-b2354f71efaf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "df=pd.read_csv(\"/content/drive/My Drive/M4R/Edery1.csv\")\n",
        "df1=pd.read_csv(\"/content/drive/My Drive/M4R/papers2019_200000.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWYn7om2HMC"
      },
      "source": [
        "# List of categories\n",
        "catlist=['math','cs','quant-ph','hep-ex','hep-th','astro-ph','nucl-ex','nucl-th']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-yCJHhMhrVZ"
      },
      "source": [
        "cat1=catlist[0]\n",
        "cat2=catlist[2]\n",
        "cat3=catlist[4]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySlUEWDLh9Yf"
      },
      "source": [
        "#Read samples from arXiv dataset for correct word to number mapping\n",
        "d1=df1[df1['categories']==cat1][0:1500].reset_index(drop=True)\n",
        "d2=df1[df1['categories']==cat2][0:1500].reset_index(drop=True)\n",
        "d3=df1[df1['categories']==cat3][0:1500].reset_index(drop=True)\n",
        "text1=d1.append(d2).reset_index(drop=True)\n",
        "text1=text1.append(d3).reset_index(drop=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoQgW_eeIs8"
      },
      "source": [
        "# Obtain corresponding abstarcts from the author\n",
        "# For Ariel Edery, all abstreacts are in at least 1 on the 3 categories, so we take all\n",
        "text=df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfsuiOZUgM4N"
      },
      "source": [
        "RANDOM_STATE = 50\n",
        "EPOCHS =15\n",
        "BATCH_SIZE = 256\n",
        "TRAINING_LENGTH = 50\n",
        "TRAIN_FRACTION = 0.7\n",
        "LSTM_CELLS = 64\n",
        "VERBOSE = 1\n",
        "SAVE_MODEL = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVrzzl8LfLzi"
      },
      "source": [
        "def format_text(text):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "\n",
        "    # Add spaces around punctuation\n",
        "    text = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', text)\n",
        "\n",
        "    # Remove references to figures\n",
        "    text = re.sub(r'\\((\\d+)\\)', r'', text)\n",
        "\n",
        "\n",
        "    # Remove meaningless words\n",
        "    text = re.sub(' the ',' ',text)\n",
        "    text = re.sub(' of ',' ',text)\n",
        "    text = re.sub(' and ',' ',text)\n",
        "    text = re.sub(' a ',' ',text)\n",
        "    text = re.sub(' in ',' ',text)\n",
        "    text = re.sub(' to ',' ',text)\n",
        "    text = re.sub(' with ',' ',text)\n",
        "    text = re.sub(' for ',' ',text)\n",
        "    text = re.sub(' by ',' ',text)\n",
        "    text = re.sub(' on ',' ',text)\n",
        "    text = re.sub(' as ',' ',text)\n",
        "    text = re.sub(' an ',' ',text)\n",
        "    text = re.sub(' at ',' ',text)\n",
        "    text = re.sub(' we ',' ',text)\n",
        "    text = re.sub(' is ',' ',text)\n",
        "    text = re.sub(' this ',' ',text)\n",
        "    text = re.sub(' are ',' ',text)\n",
        "    text = re.sub(' which ',' ',text)\n",
        "    text = re.sub(' be ',' ',text)\n",
        "    text = re.sub(' it ',' ',text)\n",
        "    text = re.sub(' that ',' ',text)\n",
        "    text = re.sub(' from ',' ',text)\n",
        "    text = re.sub(' can ',' ',text)\n",
        "    text = re.sub(' these ',' ',text)\n",
        "    text = re.sub(' our ',' ',text)\n",
        "    text = re.sub(' has ',' ',text)\n",
        "    text = re.sub(' have ',' ',text)\n",
        "    text = re.sub('.We ','.',text)\n",
        "    text = re.sub('.That ','.',text)\n",
        "    text = re.sub('.The ','.',text)\n",
        "    text = re.sub('.From ','.',text)\n",
        "    text = re.sub('.Our ','.',text)\n",
        "    text = re.sub('.In ','.',text)\n",
        "    text = re.sub('.These ','.',text)\n",
        "    text = re.sub('.This ','.',text)\n",
        "    text = re.sub(',that ',',',text)\n",
        "\n",
        "    # Remove double spaces\n",
        "    text = re.sub(r'\\s\\s', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJOim_TefQNf"
      },
      "source": [
        "formatted = []\n",
        "\n",
        "# Iterate through all the original abstracts\n",
        "for a in text['abstracts']:\n",
        "    formatted.append(format_text(a))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ0vuhnRiC61"
      },
      "source": [
        "formatted1=[]\n",
        "# Iterate through all the original abstracts\n",
        "for a in text1['abstracts']:\n",
        "    formatted1.append(format_text(a))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux4RWD5afT4Y"
      },
      "source": [
        "def make_sequences(texts1,texts,training_length=50,lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts1)\n",
        "\n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    features = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    categories=[cat1,cat2,cat3]\n",
        "    # True labels are not important\n",
        "    labels=np.zeros((len(text),len(categories)))\n",
        "    return word_idx, idx_word, num_words, word_counts, features,labels\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5n2ArgLDf3qa",
        "outputId": "9d5d1a61-2b0c-44f7-e5e2-b0398d3bae53"
      },
      "source": [
        "TRAINING_LENGTH = 50\n",
        "filters = '.,!\"#$%&()*+/:<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "word_idx, idx_word, num_words, word_counts,features,labels = make_sequences(\n",
        "    formatted1,formatted, TRAINING_LENGTH, lower=True, filters=filters)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 22110 unique words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "C774IyFfou_D",
        "outputId": "bd6e65d6-fabc-43ec-d52b-4c2aa7bca613"
      },
      "source": [
        "df1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>categories</th>\n",
              "      <th>abstracts</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>astro-ph</td>\n",
              "      <td>We systematically explore the evolution of t...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>math</td>\n",
              "      <td>Cofibrations are defined in the category of ...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>astro-ph</td>\n",
              "      <td>We explore the effect of an inhomogeneous ma...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>gr-qc</td>\n",
              "      <td>This paper has been removed by arXiv adminis...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>astro-ph</td>\n",
              "      <td>The most massive elliptical galaxies show a ...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248749</th>\n",
              "      <td>333099</td>\n",
              "      <td>adap-org</td>\n",
              "      <td>Consider the evolution $$ \\frac{\\pl m_\\iy}{\\...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248750</th>\n",
              "      <td>333100</td>\n",
              "      <td>nlin</td>\n",
              "      <td>Consider the evolution $$ \\frac{\\pl m_\\iy}{\\...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248751</th>\n",
              "      <td>333101</td>\n",
              "      <td>hep-th</td>\n",
              "      <td>Consider the evolution $$ \\frac{\\pl m_\\iy}{\\...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248752</th>\n",
              "      <td>333102</td>\n",
              "      <td>solv-int</td>\n",
              "      <td>A general solution to the Complex Monge-Amp\\...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248753</th>\n",
              "      <td>333103</td>\n",
              "      <td>nlin</td>\n",
              "      <td>A general solution to the Complex Monge-Amp\\...</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>248754 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  ...  year\n",
              "0                0  ...  2019\n",
              "1                1  ...  2019\n",
              "2                2  ...  2019\n",
              "3                3  ...  2019\n",
              "4                4  ...  2019\n",
              "...            ...  ...   ...\n",
              "248749      333099  ...  2019\n",
              "248750      333100  ...  2019\n",
              "248751      333101  ...  2019\n",
              "248752      333102  ...  2019\n",
              "248753      333103  ...  2019\n",
              "\n",
              "[248754 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZapiM5oFLCY"
      },
      "source": [
        "#Make all sequences to same length\n",
        "pad = len(max(features, key=len))\n",
        "features=[i + [0]*(pad-len(i)) for i in features]\n",
        "# Reshape to an array\n",
        "features1=np.array(features)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebHYFmyOw5SP"
      },
      "source": [
        "# Define Training and Test sets\n",
        "X_test=features1\n",
        "\n",
        "y_test=labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lUEtBKFuJJq"
      },
      "source": [
        "# Load in GloVe\n",
        "glove=np.loadtxt('/content/drive/My Drive/M4R/glove.6B.100d.txt',dtype='str', comments=None)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpp0cJNyuOOb"
      },
      "source": [
        "# Split words from vector representation\n",
        "vectors = glove[:, 1:].astype('float')\n",
        "words = glove[:, 0]\n",
        "del glove"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I90tWTduVVF",
        "outputId": "b1998e90-e782-470f-8aeb-61906c4ca242"
      },
      "source": [
        "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
        "\n",
        "not_found = 0\n",
        "\n",
        "for i, word in enumerate(word_idx.keys()):\n",
        "    # Look up the word embedding\n",
        "    vector = word_lookup.get(word, None)\n",
        "\n",
        "    # Record in matrix\n",
        "    if vector is not None:\n",
        "        embedding_matrix[i + 1, :] = vector\n",
        "    else:\n",
        "        not_found += 1\n",
        "\n",
        "print(f'There were {not_found} words without pre-trained embeddings.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 9126 words without pre-trained embeddings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLvzmm1cuY3D",
        "outputId": "64fdf8ea-f468-4b45-af91-a449dade06e0"
      },
      "source": [
        "import gc\n",
        "gc.enable()\n",
        "del vectors\n",
        "gc.collect()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "161"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqkaoBQrucAV",
        "outputId": "68267014-f3b1-42bc-da90-030b57140f3f"
      },
      "source": [
        "# Normalize and convert nan to 0\n",
        "embedding_matrix = embedding_matrix / \\\n",
        "    np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
        "embedding_matrix = np.nan_to_num(embedding_matrix)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3dn_LTQAEiz"
      },
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v01BrdD2_As2",
        "outputId": "aaffa50e-6230-4953-da6a-686e3f5cedfd"
      },
      "source": [
        "def make_word_level_model(embedding_matrix,\n",
        "                          lstm_cells=64,\n",
        "                          trainable=False,\n",
        "                          lstm_layers=1,\n",
        "                          bi_direc=False):\n",
        "    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n",
        "       and varying numbers of LSTM cell layers.\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Map words to an embedding\n",
        "    if not trainable:\n",
        "        model.add(\n",
        "            Embedding(\n",
        "                input_dim=num_words,\n",
        "                output_dim=embedding_matrix.shape[1],\n",
        "                weights=[embedding_matrix],\n",
        "                trainable=False,\n",
        "                mask_zero=True))\n",
        "        model.add(Masking())\n",
        "    else:\n",
        "        model.add(\n",
        "            Embedding(\n",
        "                input_dim=num_words,\n",
        "                output_dim=embedding_matrix.shape[1],\n",
        "                weights=[embedding_matrix],\n",
        "                trainable=True))\n",
        "\n",
        "    # If want to add multiple LSTM layers\n",
        "    if lstm_layers > 1:\n",
        "        for i in range(lstm_layers - 1):\n",
        "            model.add(\n",
        "                LSTM(\n",
        "                    lstm_cells,\n",
        "                    return_sequences=True,\n",
        "                    dropout=0.1,\n",
        "                    recurrent_dropout=0.1))\n",
        "\n",
        "    # Add final LSTM cell layer\n",
        "    model.add(\n",
        "        LSTM(\n",
        "            lstm_cells,\n",
        "            return_sequences=False,\n",
        "            dropout=0.1,\n",
        "            recurrent_dropout=0.1))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # Dropout for regularization\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(len(labels[0]), activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = make_word_level_model(\n",
        "    embedding_matrix=embedding_matrix,\n",
        "    lstm_cells=LSTM_CELLS,\n",
        "    trainable=False,\n",
        "    lstm_layers=2,\n",
        "    bi_direc=False)\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         2211000   \n",
            "_________________________________________________________________\n",
            "masking (Masking)            (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 64)          42240     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 2,294,971\n",
            "Trainable params: 83,971\n",
            "Non-trainable params: 2,211,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFNeVWZRAS4C"
      },
      "source": [
        "from IPython.display import Image\n",
        "model_name = 'pre-trained-rnn'\n",
        "model_dir = '../models/'\n",
        "# Plot model is no is functioning due to updeates of Keras\n",
        "#plot_model(model,show_shapes=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oK8UN_AenvH"
      },
      "source": [
        "# Load and evaluate model\n",
        "def load_and_evaluate(model_name, return_model=False):\n",
        "    \"\"\"Load in a trained model and evaluate with log loss and accuracy\"\"\"\n",
        "\n",
        "    model = load_model(f'/content/drive/My Drive/M4R/3cat642layeredery.h5')\n",
        "\n",
        "    if return_model:\n",
        "        return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBUUiTnIe4ag"
      },
      "source": [
        "model = load_and_evaluate(model_name, return_model=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWOsxYKR5Egh",
        "outputId": "4489b007-df99-420f-9bad-887516821262"
      },
      "source": [
        "prob=model.predict(X_test)\n",
        "count=[0,0,0]\n",
        "for i in range(len(prob)):\n",
        "  if max(prob[i,:])==prob[i,0]:\n",
        "    count[0]+=1\n",
        "  elif max(prob[i,:])==prob[i,1]:\n",
        "    count[1]+=1\n",
        "  else:\n",
        "    count[2]+=1\n",
        "print(count)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 0, 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YppH7jSgebvw",
        "outputId": "a3e1628d-7144-4276-a5d8-52ee421bfb95"
      },
      "source": [
        "prob"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03415843, 0.47351778, 0.49232382],\n",
              "       [0.02133512, 0.04538091, 0.9332839 ],\n",
              "       [0.035587  , 0.01183971, 0.9525733 ],\n",
              "       [0.04646597, 0.01496865, 0.9385654 ],\n",
              "       [0.07370985, 0.03493878, 0.8913514 ],\n",
              "       [0.08898628, 0.01232543, 0.89868826],\n",
              "       [0.0960291 , 0.0355242 , 0.8684467 ],\n",
              "       [0.07719919, 0.0292128 , 0.89358807],\n",
              "       [0.02901187, 0.01335072, 0.9576374 ],\n",
              "       [0.2630705 , 0.01111116, 0.72581834],\n",
              "       [0.02342292, 0.01323372, 0.9633434 ],\n",
              "       [0.03195693, 0.01069648, 0.9573466 ],\n",
              "       [0.06419756, 0.05810872, 0.8776938 ],\n",
              "       [0.06076048, 0.01181464, 0.92742485],\n",
              "       [0.0274606 , 0.022265  , 0.9502744 ],\n",
              "       [0.03557672, 0.00946059, 0.9549627 ],\n",
              "       [0.04899297, 0.04922823, 0.9017788 ],\n",
              "       [0.03168702, 0.01949646, 0.9488165 ],\n",
              "       [0.02601572, 0.01552891, 0.9584553 ],\n",
              "       [0.02736058, 0.03768641, 0.93495303],\n",
              "       [0.08955088, 0.0705959 , 0.8398532 ],\n",
              "       [0.3287405 , 0.32160687, 0.34965268],\n",
              "       [0.90076846, 0.02969038, 0.06954123],\n",
              "       [0.08281455, 0.04546921, 0.8717162 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    }
  ]
}